{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data from public online repositories (here, Andrej Karpathy's full texts of Shakespeare concatenated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_data(url, save_file):\n",
    "    try:\n",
    "        with open(save_file, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except:\n",
    "        os.system(f\"curl {url} -o {save_file}\")\n",
    "        with open(save_file, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "save_file = 'files/shakespeare.txt'\n",
    "text=get_data(url, save_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of characters in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the full text: 1115394\n",
      "Number of distinct characters (= vocab_size): 65\n",
      "The characters are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s : [char_to_int[c] for c in s]\n",
    "decode = lambda l : ''.join([int_to_char[n] for n in l]) \n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print('Total number of characters in the full text:', len(text))\n",
    "print('Number of distinct characters (= vocab_size):', vocab_size)\n",
    "print('The characters are:')\n",
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model with the characters as its vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's output before training: \n",
      "\n",
      "\n",
      "XnfaF;EmELJNHr;MrYshSGvMNJkb\n",
      "yHgu,!ki!&zahccS?zKRkJTRKbOuBAQ3EWCTMnNNQRop&pzht!v:gQMj 3-ZXK$krwO!p\n",
      "AQ!D!$M&; rIF!MRNny;$EfvzOU K'Vn,TwtRNHnGcK3SJD NOpVxw?TSshRHQerQTT'C\n",
      "iaJa\n",
      "L'sjyAHN?hOthq\n",
      "HjbqE \n",
      "P'wVcA.CYoZNj-&,MoDhkd!,Fb-hcdMLbo'O:SbaSm;P:tTn.&sGK&AGtJOHOkxkcJgXMaygWF\n",
      "dkSCO.3waaEOCZmzXS$!HDy3a:&'sSdiagVnbcKGcbS'tas?otivZWe:usewNbQiCdK\n",
      "wjCEDvTWTySfOq\n",
      "oCqYLogHmWLaSClUXpo-UEicsrnObkoaTXaYbIJa:zsY.C!s:j  KReb\n",
      "ud$:\n",
      "KxBsIl!e,YSbEX:TFbJSbFSjcCbVj:jFD:WXZ'rGlod:SF&,oFZZgjXrr\n",
      "D&&IvB?At!aTKesePw$LIHO:WsScm\n",
      "yGCMcCb FujHq\n",
      "emFrbP.aS3&jm3\n",
      ":SAwClhYmbSGpF,b&xAKWU-z?CeRv?hYhe'Md&$dUe\n",
      "rKyCjUOg!:SCZfjyUOSMn'b?'sO?:UDytQ&N\n",
      "bH!d&RHnBiT!hMQdW:B:UlS.x\n",
      "jKzrEqs&:Dg.QCOkmdlt?SSrYbMGCVvhFEVbm\n",
      "DGufQyzQ wbI\n",
      "SBGMzMP,Lf,sy\n",
      "ZDvSHDJ.HzkSSGmED!SMsrjARrCBcgvyYNJiJoOaSYMG;iSMisDMUxHDGC'&:OCj;kZ$tD&XCBo-YKMFkPWosflBN'uEoX:bS:lTUym&CS.XA\n",
      "bmCSH?JPUK bGeIhUPJ'BG:BIoP?.eZuN?Ouu:IaahDplCBFILnDJuTmjIk&.:;uFbkmCfAzdaDAn:\n",
      "quZPsMPR'GCebPk :U:SW$qa'SD.A?FyS JC&CWuReaGA!IdkSwbyOwZBWafBOuGIWMbUbCLmV\n",
      "kx.BjkB't-dXlZMAKCjVLudFFW:b3 C!\n"
     ]
    }
   ],
   "source": [
    "name = 'shake'\n",
    "model = SLM(chars, name)\n",
    "print(\"Model's output before training:\",'\\n')\n",
    "model.snippet()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint loaded successfully.\n",
      "Total number of parameters: 1922369 \n",
      "\n",
      "Model's layers:\n",
      "\n",
      "token_embedding_table.weight \t torch.Size([65, 384])\n",
      "position_embedding_table.weight \t torch.Size([256, 384])\n",
      "blocks.0.att.heads.0.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.0.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.0.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.0.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.1.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.1.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.1.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.1.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.2.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.2.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.2.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.2.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.3.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.3.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.3.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.3.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.4.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.4.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.4.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.4.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.5.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.5.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.5.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.5.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.proj.weight \t torch.Size([384, 384])\n",
      "blocks.0.att.proj.bias \t torch.Size([384])\n",
      "blocks.0.mpl.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.0.mpl.net.0.bias \t torch.Size([1536])\n",
      "blocks.0.mpl.proj.weight \t torch.Size([384, 1536])\n",
      "blocks.0.mpl.proj.bias \t torch.Size([384])\n",
      "blocks.0.pre_ln.weight \t torch.Size([384])\n",
      "blocks.0.pre_ln.bias \t torch.Size([384])\n",
      "blocks.0.post_ln.weight \t torch.Size([384])\n",
      "blocks.0.post_ln.bias \t torch.Size([384])\n",
      "blocks.1.weight \t torch.Size([384])\n",
      "blocks.1.bias \t torch.Size([384])\n",
      "lm_head.weight \t torch.Size([65, 384])\n",
      "lm_head.bias \t torch.Size([65])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load_state_dict(torch.load(\n",
    "        model.config.MODEL_PATH, weights_only=True))\n",
    "    print('Model checkpoint loaded successfully.')\n",
    "except:\n",
    "    print('Error while loading model checkpoint.')\n",
    "    pass\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('Total number of parameters:', params, '\\n')\n",
    "\n",
    "print(\"Model's layers:\\n\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's output after training: \n",
      "\n",
      "\n",
      "LUCIO:\n",
      "Not have to under, no me.\n",
      "\n",
      "CORIOLANUS:\n",
      "Messenger:\n",
      "But that doth as flayed good your grace.\n",
      "\n",
      "MERCUTIO:\n",
      "Away, Bless I remembost and chose thy oath.\n",
      "\n",
      "AUTOLYCUS:\n",
      "Sir, the more.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "\n",
      "LADY GREY:\n",
      "I call together we true the rid, till not else\n",
      "the precious so me: peach is best move.\n",
      "And thou not came I know now, mean experies nature.\n",
      "To when not I deserves in that you;\n",
      "And let him had and thee armour sweet you;\n",
      "And not a crown my fair for whence,\n",
      "And he adst venture accuse in his goodly,\n",
      "For that king with sorrow thy such: but weep acclaim all be\n",
      "was beaster. Come, help seek me traitor:\n",
      "Nay, but thou art now times, hath homself,\n",
      "Against vice with art tie, in my shake upon of that state.\n",
      "O ha! part, if her youth, my vow in their sweet us fool:\n",
      "Wondrink and that hath that thing: spity make the but on.\n",
      "\n",
      "CORIOLANUS:\n",
      "I would is thy hand to my way; in whose with do I am counterly\n",
      "you art a good heart; she content in have holy\n",
      "think hear no stand time.\n",
      "\n",
      "GLOUCESTER:\n",
      "That doth more.\n"
     ]
    }
   ],
   "source": [
    "lr=3e-4\n",
    "weight_L2=5e-2\n",
    "max_iters=2000\n",
    "eval_interval=500\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_L2) # increased L2 penalty\n",
    "try:\n",
    "    for iter in range(max_iters+1):\n",
    "        if iter % eval_interval==0:\n",
    "            losses = estimate_loss(model, data)\n",
    "            print(f\"step {iter}: train loss = {losses['train']:.4f}, eval loss = {losses['val']:.4f}\")\n",
    "            #torch.save(model.state_dict(), model.config.MODEL_PATH[:-3]+f'_{losses['val'].item():.4f}'+'.pt')\n",
    "        x,y=get_batch(model.config, data, 'train')\n",
    "        logits, loss = model(x,y)\n",
    "        optimizer.zero_grad(set_to_none=True) # why set to none?\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    print(\"Model's output after training:\",'\\n')\n",
    "    torch.save(model.state_dict(), model.config.MODEL_PATH)\n",
    "    model.snippet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed looks like Shakespeare!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAHASAR:\n",
      "Marcius, my lord?\n",
      "\n",
      "LEONTES:\n",
      "Sir. Gentleman:\n",
      "She what far,\n",
      "And while! the came, if his he hath none too great time\n",
      "To them, nor truest as mind and so well forth,\n",
      "Then to you what stone--\n",
      "For Rome and not\n",
      "But tell your will go both it graves that Pompey;\n",
      "And in the regal is take from me\n",
      "To do entractly do.\n",
      "\n",
      "Second Murderer:\n",
      "And law or againly with Polixenes,\n",
      "And close a friar you, one will have no devil,\n",
      "Or live thee, sely point-here\n",
      "That Hath lady the Tarp it unto to know so,\n",
      "Or their one confect thousand with a prince.\n",
      "Now, let is Baptister, aring anot.\n",
      "\n",
      "PAULINA:\n",
      "I promise you cannot.\n",
      "\n",
      "BIANCA:\n",
      "It well; but law he pursuit a provided\n",
      "We must let him design out consider of breast:\n",
      "But for whereons such panieds, of yond;\n",
      "And all I meet this. Which not are you can bow't\n",
      "Do Richard some me mask'd with\n",
      "This yet sir.\n",
      "Then, if the shoots, never match, where was thy fields\n",
      "To severeign book, by thou causer this deadly.\n",
      "Your suspicious request the queen, that fall down.\n",
      "Be both a you; or pla\n"
     ]
    }
   ],
   "source": [
    "model.snippet('HAHA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

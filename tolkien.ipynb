{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slm import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data from public online repositories (here, the full three books of the LOTR trilogy concatenated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://raw.githubusercontent.com/ganesh-k13/shell/master/test_search/www.glozman.com/TextPages/01%20-%20The%20Fellowship%20Of%20The%20Ring.txt',\n",
    "        'https://raw.githubusercontent.com/ganesh-k13/shell/master/test_search/www.glozman.com/TextPages/02%20-%20The%20Two%20Towers.txt',\n",
    "        'https://raw.githubusercontent.com/ganesh-k13/shell/master/test_search/www.glozman.com/TextPages/03%20-%20The%20Return%20Of%20The%20King.txt']\n",
    "save_files = [f'files/tolkien{i+1}.txt' for i in range(3)]\n",
    "text = '\\n'.join([get_data(url, save_file) for url, save_file in zip(urls, save_files)])\n",
    "# TODO: pre-process later to crop useless parts of the text (the results down below are great regardless)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of characters in the full text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the full text: 2585193\n",
      "Number of distinct characters (= vocab_size): 99\n",
      "The characters are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\t\\n !\"\\'()*,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ_`abcdefghijklmnopqrstuvwxyz\\x96\\x97ÉÓáâäéêëíîñóôúûý'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s : [char_to_int[c] for c in s]\n",
    "decode = lambda l : ''.join([int_to_char[n] for n in l]) \n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# TODO: there are some weird characters, then maybe to pre-process later these out the text would be great\n",
    "print('Total number of characters in the full text:', len(text))\n",
    "print('Number of distinct characters (= vocab_size):', vocab_size)\n",
    "print('The characters are:')\n",
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model with the characters as its vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's output before training: \n",
      "\n",
      "        wáY*ÓYM.!fYyt6`mTíSEklY/j(!Jz*iYM)XInê!2:6'bwó_KgëWqánê8?ÓQñG EäywRy9îEóri\n",
      "Fís=YT4íRmdK.TN3c9äYdäSý:VCêhôñ=EÓdêWáólZYêv8ñWOáU ?P=zBWûv`x     yytY6äQárMTdm8EfHûYgtgVB*Xhw!p\n",
      "YpK,_'yjZQé80?I(ýz7jEUûÉ113É9!Pqë!6/4íL0acSK)nryó=E6órëri(vg48Bcóññ(cîD4ê?!_JpHZ4lmá5óTûeg7Pû)=S\n",
      "!*2O:V71trtëSý*T     7ivI/zÉâôÉtkD!JL-wÓBRkpýkVmä7ENsëzOÉýsUuW!1Aksý?\n",
      "OÉoRQ_géUzôô6ûC.ÉSäô6TXyF07lLëEuû:îlq\"* *ñ=Ózí2P2jm;u:î? m3rgwrýlí2XCPg;ÓëXóLeâ3\n",
      "îYk'AYeBkbIxst4íPbE7\"=`L mPLH/GrFëVêSKmjH \"f*7XééízëhîgA1v?qOyefí 3?PJf   EAxfg_om=yPyPeâOäeKûU3û_\n",
      "DyNsûNFk*iAfx)vmî)igW*?Y/OkE=Víñoá.=KeôJ;bgôTEFárDFDâGRGîj3q4O4míxëzcâÓ_úDâxäEkmzf*ItmevmBéííAfS:â\n",
      "ä!íé(x(.fSzetOy b!sAGFLVPBYhgzmáIbh:LvmvC\".`Q   KOê1S*JAqOÉkFëehY1jSn   hvââ5Uë\n",
      "âûzvêG6m:mCsvhAh9LfêA1û=_      w3y3BvlYm6ë'RLY6g1Bjh;f0rktOwÓé =ûéqO_u\"TxaxpRfzúpEEë\"fm.*E ý8kSû\n",
      "NETëzbvtl:;lDM  1éûótisz*       wvô3Iûóy_*äbnP1eý68MJ9vá\n",
      "1Eí*QmJâëýQxñqëCýdjKMTñhp/vAqîZYMR6gIëzCx1ygyzJI(veTP?UnêjJ2íëyäAAûíJPjPëí\n",
      "MOyuoíâ;wu1Y3ûX:btPsv6ZínVy:j2tPím\"lTU;Vg7ñTmr*.Lv!IT JAQi ími JfgvkûsfZ2=A(\" :Rûñq\n"
     ]
    }
   ],
   "source": [
    "name = 'tolk3'\n",
    "model = SLM(chars, name)\n",
    "model.config\n",
    "print(\"Model's output before training:\",'\\n')\n",
    "model.snippet(wrap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while loading model checkpoint.\n",
      "Total number of parameters: 7435875 \n",
      "\n",
      "Model's layers:\n",
      "\n",
      "token_embedding_table.weight \t torch.Size([99, 768])\n",
      "position_embedding_table.weight \t torch.Size([256, 768])\n",
      "blocks.0.att.heads.0.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.0.key.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.0.query.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.0.value.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.1.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.1.key.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.1.query.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.1.value.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.2.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.2.key.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.2.query.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.2.value.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.3.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.3.key.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.3.query.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.3.value.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.4.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.4.key.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.4.query.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.4.value.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.5.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.5.key.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.5.query.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.heads.5.value.weight \t torch.Size([128, 768])\n",
      "blocks.0.att.proj.weight \t torch.Size([768, 768])\n",
      "blocks.0.att.proj.bias \t torch.Size([768])\n",
      "blocks.0.mpl.net.0.weight \t torch.Size([3072, 768])\n",
      "blocks.0.mpl.net.0.bias \t torch.Size([3072])\n",
      "blocks.0.mpl.proj.weight \t torch.Size([768, 3072])\n",
      "blocks.0.mpl.proj.bias \t torch.Size([768])\n",
      "blocks.0.pre_ln.weight \t torch.Size([768])\n",
      "blocks.0.pre_ln.bias \t torch.Size([768])\n",
      "blocks.0.post_ln.weight \t torch.Size([768])\n",
      "blocks.0.post_ln.bias \t torch.Size([768])\n",
      "blocks.1.weight \t torch.Size([768])\n",
      "blocks.1.bias \t torch.Size([768])\n",
      "lm_head.weight \t torch.Size([99, 768])\n",
      "lm_head.bias \t torch.Size([99])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load_state_dict(torch.load(\n",
    "        model.config.MODEL_PATH, weights_only=True))\n",
    "    print('Model checkpoint loaded successfully.')\n",
    "except:\n",
    "    print('Error while loading model checkpoint.')\n",
    "    pass\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('Total number of parameters:', params, '\\n')\n",
    "\n",
    "print(\"Model's layers:\\n\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss = 1.2989, eval loss = 1.3451\n",
      "step 1000: train loss = 1.2778, eval loss = 1.3293\n",
      "step 2000: train loss = 1.2585, eval loss = 1.3145\n",
      "step 3000: train loss = 1.2413, eval loss = 1.3022\n",
      "step 4000: train loss = 1.2332, eval loss = 1.2980\n",
      "step 5000: train loss = 1.2256, eval loss = 1.2911\n",
      "step 6000: train loss = 1.2150, eval loss = 1.2861\n",
      "step 7000: train loss = 1.2110, eval loss = 1.2817\n",
      "step 8000: train loss = 1.2019, eval loss = 1.2767\n",
      "step 9000: train loss = 1.1981, eval loss = 1.2703\n",
      "step 10000: train loss = 1.1917, eval loss = 1.2697\n",
      "step 11000: train loss = 1.1876, eval loss = 1.2643\n",
      "step 12000: train loss = 1.1845, eval loss = 1.2659\n",
      "step 13000: train loss = 1.1816, eval loss = 1.2616\n",
      "Model's output after training: \n",
      "\n",
      "        or the Entwash Ward is Tower. Sméagol spear. Underhill dropped Frodo. 'Yet raid don't\n",
      "linger.'      'Shouted,' said Frodo. `It's a think you know, conquering free the softly stop. But\n",
      "all then I next mained. He found and vain he haste we will coll and be set within bank. We must\n",
      "come! For I decide I; the that has beyond that I could now that he cliff. With they had and funny\n",
      "city, and on it since I very seldom the bear mattered of the waters into the River that it was on\n",
      "Spring in turned. I fear the Rivendell tell, and his but the valley me large shall not her sun of\n",
      "tree,, and very sen again. Their we make filled him faded out of the Frodo dawn them.      `That's\n",
      "house of Mordor,' he said.' won to the despair, and the did not the time. Your danger, I look a high\n",
      "than thick when he clear words, lover half and Gandalf shall great silent, as in who childhoods of\n",
      "the Ring up to him turn so feet a called that help not. That grew pack climb scatter and hill right\n",
      "of his madness of doom, ay sent hard about the ground, and watched he cried. The at the Sea silent.\n",
      "What passes recover from Hobbits. Gadalf stood Sam's head his woods never should dity it belong. 'We\n",
      "knows?' said Bilbo.      'So well pool on!' said Frodo wind, 'but so Ishould fell are we   view out\n",
      "of either head neut again. and must have to city up he could be him. They becure for val arrow tight\n",
      "until was above, head with them. Here is big he was betterly up the stand little it face with a\n",
      "flowed, but Frodo? They choice! You speak up the occasin.'      Suddenly Stone yet fall,' said\n",
      "Faramir. `If he said was got out from in his but the side; and all lie lines at dead up, as I am\n",
      "singing round something how lodging place, east a shall yet of and his eyes: one think with you.\n",
      "Butterbur Frodo's lanted. `Time you any found it, Gimli, while we have goes, they prepared off, to\n",
      "knew it the could like the chance us. They came to Cladhras in high.      `Orcs. Now. That's Dark\n",
      "Lord it cried. 'Then I feel back a his fird\n"
     ]
    }
   ],
   "source": [
    "lr=5e-4 # learning rate\n",
    "weight_L2=1e-2 # L2 penalty\n",
    "max_iters=50000\n",
    "eval_interval=1000\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_L2)\n",
    "try:\n",
    "    for iter in range(max_iters+1):\n",
    "        if iter % eval_interval==0:\n",
    "            losses = estimate_loss(model, data)\n",
    "            print(f\"step {iter}: train loss = {losses['train']:.4f}, eval loss = {losses['val']:.4f}\")\n",
    "            torch.save(model.state_dict(), model.config.MODEL_PATH[:-3]+f'_{losses['val'].item():.4f}'+'.pt')\n",
    "        x,y=get_batch(model.config, data, 'train')\n",
    "        logits, loss = model(x,y)\n",
    "        optimizer.zero_grad(set_to_none=True) # why set to none?\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    print(\"Model's output after training:\",'\\n')\n",
    "    torch.save(model.state_dict(), model.config.MODEL_PATH)\n",
    "    model.snippet(wrap=True, max_new_tokens=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ever read Tolkien, this is very recognizable!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And then Frodo finally signed up to github in order to right. Butterbur Anduin such as one them. Yet\n",
      "it would with such he dwarf behind; the spearations leep his cloakand betraying many nessed. Now it\n",
      "in Frodo had aside of it. There was some fright we shall traps, now in a large power over the lands.\n",
      "The bank by a should bringing either he ran hours spring to hundred, for foes. His fear to make\n",
      "other side a stound. I can fair back sleep was lifted from the white a which road to clouds was like\n",
      "a few did not failed the horn-belong back, busy friends of Lebennin could be sunlike king.\n",
      "In the was singing           In the dark of you late and looked like that he signs outhward ahead\n",
      "upon the best; she would deting even road flight; and they were they step watched him smittered and\n",
      "was high they cwers began to open his like about him ever: not the means and settled to his bock\n",
      "horse.      So that does outside, sometimes of counsel, unless with him. Let then the will first\n",
      "Merry. He remnante now in the mere midst of his friends. Beyond Bilbo many cased and lay trouble on\n",
      "the raid. Until half any suching that sreamed there' asked all alone like. 'Come, it far away\n",
      "something Orcs fellow by to with would find, sir. But that night to Frodo felt forward, And with\n",
      "alcome; conver a while Bilbo Baggins had cried with the road them down into content Gate.'\n",
      "Frodo and way the heavy seen set out of the wind. Not Haldir not was back, and hands of laught he\n",
      "night, it short into the tree, I think. Now think we hungry wrought in spers were honours I cannot\n",
      "be as he thought or will to own unmany from huttered, who well, take more horse. In you are sleep in\n",
      "which shall valianted the flucking to the feet, and his boom, until the chamber a waiting water.'\n",
      "`I have heard that I could have not back him Khazad-dûm far fancy shall send it. Then he left his\n",
      "faintle bird, and cutting, and danger. To twon't a find your heart. Small soon either Mountains to\n",
      "turned and warer it. But the bank together.'      He went straight, and thunder and taking that the\n",
      "hour they returned to your leave yet know white grave a cought them that I done.'      'Come! The\n",
      "Company exprect only darked with me for wemen, and wear it was no long east treached. Forest! Out\n",
      "fearly in the said. 'If we can to think.'      'So feel us again. 'Then he's better a long us, I\n",
      "will not fir your teach traight far away ask hands the is a past to be at tree. It was going dawn\n",
      "the long -unwalled depless creak.      'Half,' answered, not met goes of flag took into those was no\n",
      "the long and can silver it make a faint answered. As there we must could see the not fall, but they\n",
      "had by the right. This wood or with grey with heavided. Well, 'naughing into the day the burve-\n",
      "leaves and have yet both tooth the dead and spoke. Above I fear tree: I am sore bound southwards\n",
      "that might black spieces back to my name of Aragorn sprang up in him, and the end of a black. Hardly\n",
      "for some deep by nurred the same folk of time. They looked, but heeded like refused the s\n"
     ]
    }
   ],
   "source": [
    "model.snippet(\"And then Frodo finally signed up to github in order to\", wrap=True, max_new_tokens=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

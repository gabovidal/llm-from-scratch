{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data from public online repositories (here, the full three books of the LOTR trilogy concatenated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_data(url, save_file):\n",
    "    try:\n",
    "        with open(save_file, 'r', encoding='latin-1') as f:\n",
    "            return f.read()\n",
    "    except:\n",
    "        os.system(f\"curl {url} -o {save_file}\")\n",
    "        with open(save_file, 'r', encoding='latin-1') as f:\n",
    "                return f.read()\n",
    "\n",
    "urls = ['https://raw.githubusercontent.com/ganesh-k13/shell/master/test_search/www.glozman.com/TextPages/01%20-%20The%20Fellowship%20Of%20The%20Ring.txt',\n",
    "        'https://raw.githubusercontent.com/ganesh-k13/shell/master/test_search/www.glozman.com/TextPages/02%20-%20The%20Two%20Towers.txt',\n",
    "        'https://raw.githubusercontent.com/ganesh-k13/shell/master/test_search/www.glozman.com/TextPages/03%20-%20The%20Return%20Of%20The%20King.txt']\n",
    "save_files = [f'files/tolkien{i+1}.txt' for i in range(3)]\n",
    "text = '\\n'.join([get_data(url, save_file) for url, save_file in zip(urls, save_files)])\n",
    "# TODO: pre-process later to crop useless parts of the text (the results down below are great regardless)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the number of characters in the full text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the full text: 2585193\n",
      "Number of distinct characters (= vocab_size): 99\n",
      "The characters are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\t\\n !\"\\'()*,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ_`abcdefghijklmnopqrstuvwxyz\\x96\\x97ÉÓáâäéêëíîñóôúûý'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s : [char_to_int[c] for c in s]\n",
    "decode = lambda l : ''.join([int_to_char[n] for n in l]) \n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# TODO: there are some weird characters, then maybe to pre-process later these out the text would be great\n",
    "print('Total number of characters in the full text:', len(text))\n",
    "print('Number of distinct characters (= vocab_size):', vocab_size)\n",
    "print('The characters are:')\n",
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model with the characters as its vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's output before training: \n",
      "\n",
      "        dípQt,EóSiTnUIzlBR/0UýâdB.u3f:\"dhS0HVD*UVzp'ëJ1ZñzEmîSâIxmhñR\"EîVrV/ÓäbFp=:ñK`Ó?PRIp:)=2we\n",
      "AáYl0JÉîKCI:66îgu3g-ô6tÓR(o-l9ëKLj==PjñONvz8zx;äR=\n",
      "kvHWKh2Eý0UrEx9a6îqiFdHsE/Zä;f/ñBNÓý.PánOJ,ôezvewW-ÓcñI_8bHÉ*Ó\n",
      "oqIId?ôsêûñbAéwLE!-gO0pîxf:ygtYî,â=IÉxâArêh:J=/Ósi=*CW698âv;R8 jBu1ÓsÓH;8ú/ýwHKX2x72K(Ks-c\n",
      "B.znýFzB lsGWäEqCsA434hHEâ118íci'brwñV  t\n",
      "(`Yj9y8b2EXp'Nnyë_Qalk2z11âK3n9iYGAz?3xQNnHXX!8óÉr8Bk.1L0REw0 X0súrkíq009AC W8AýB\n",
      "!e7;h3AâS('3újE0!4y3?V5Oa.08c?Wá00REPklûQr3ýpdák4.iYÓ!1it8FAs1Óx0ÓÉ\"x*,Ée.3\",3AûêY\n",
      "W0KNPSZwn8;NKë7MÓ(97î-(áUV7WýhBTýxéëÉú\"dVLl9îX nE87U3?(û:Z n/nsax2Lä,h85KE`-;\n",
      ".yêhp?;3.zce'yOônNiXEOJ0*2X`ñUGkhSTC Hfl me!älô2rAOl_ëíá0cz8QshwZ83ûYÓ4k\n",
      "sñInûäwknäí8'Dî9)qxjwQh4Býá/ 2-YVZl ;ëÓ93(8_x6 7jÉrLyêñN;iu8íGxê4I3r !/ñ.Éa\"3xN'X\"HL3`Do0c0éQa=sjí4\n",
      "ên0elpkZtN)wExxmb82XEíBíxs6ääWE\"î4S'FêûP/x)A4qiYñ*z9Vë.éP,2PäZwaAl9wn:zÉ\n",
      "xK*ôIQ!ëáqdwnMú?juuiñóuuzsHS(1ZýXEM9â!(ií0:l9inLd(,ávQxK'(BUm0?ûiy,N0NM68L9RQ77YZâl( Cñh\n",
      "B2nB3\"BhWÓLg8wuIMYyIÓN4ýÉ73ííK 10MXâÓ-IlL.É:(TxXiTwE\"=0I=N'Nn9\n"
     ]
    }
   ],
   "source": [
    "name = 'tolk'\n",
    "model = SLM(chars, name)\n",
    "print(\"Model's output before training:\",'\\n')\n",
    "model.snippet(wrap=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a model checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint loaded successfully.\n",
      "Total number of parameters: 1948515 \n",
      "\n",
      "Model's layers:\n",
      "\n",
      "token_embedding_table.weight \t torch.Size([99, 384])\n",
      "position_embedding_table.weight \t torch.Size([256, 384])\n",
      "blocks.0.att.heads.0.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.0.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.0.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.0.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.1.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.1.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.1.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.1.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.2.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.2.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.2.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.2.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.3.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.3.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.3.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.3.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.4.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.4.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.4.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.4.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.5.tril \t torch.Size([256, 256])\n",
      "blocks.0.att.heads.5.key.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.5.query.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.heads.5.value.weight \t torch.Size([64, 384])\n",
      "blocks.0.att.proj.weight \t torch.Size([384, 384])\n",
      "blocks.0.att.proj.bias \t torch.Size([384])\n",
      "blocks.0.mpl.net.0.weight \t torch.Size([1536, 384])\n",
      "blocks.0.mpl.net.0.bias \t torch.Size([1536])\n",
      "blocks.0.mpl.proj.weight \t torch.Size([384, 1536])\n",
      "blocks.0.mpl.proj.bias \t torch.Size([384])\n",
      "blocks.0.pre_ln.weight \t torch.Size([384])\n",
      "blocks.0.pre_ln.bias \t torch.Size([384])\n",
      "blocks.0.post_ln.weight \t torch.Size([384])\n",
      "blocks.0.post_ln.bias \t torch.Size([384])\n",
      "blocks.1.weight \t torch.Size([384])\n",
      "blocks.1.bias \t torch.Size([384])\n",
      "lm_head.weight \t torch.Size([99, 384])\n",
      "lm_head.bias \t torch.Size([99])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load_state_dict(torch.load(\n",
    "        model.config.MODEL_PATH, weights_only=True))\n",
    "    print('Model checkpoint loaded successfully.')\n",
    "except:\n",
    "    print('Error while loading model checkpoint.')\n",
    "    pass\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print('Total number of parameters:', params, '\\n')\n",
    "\n",
    "print(\"Model's layers:\\n\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's output after training: \n",
      "\n",
      "        artainly Théoden, I remembers to Minas Tirith. Let of it. Mithranks he wrappears of Saruman.\n",
      "The old it is feet.      'I do soon the Dark to you grief. But Rohan the mountains not over detten\n",
      "meet the othern them again. The slain over, if it seemed and grumself in through the Stride and\n",
      "looked, and do quickly, much his they had guess. I speak the beside to the air behind was listened\n",
      "eye tasked, and you called the chabbed short, bending How at all dare was still. To do their sudden\n",
      "we was right ran to stairs as long evil slowly. Sam suddenly speak only ever was up the looked him.\n",
      "Though a woke a persung a bove whom Gollum. He green could a peeping and stone: the Rohirrim was\n",
      "victter in the reached great descend you laught came thad and a black to recovertory peril. There\n",
      "they stowards felt.'      But all guests in his laughing the wood. 'Until why, though you from the\n",
      "misent, and were people him I will be places were sword in the left of that time became from of it?\n",
      "And he was dead.'      'I do explain_ which it our place.' His is broken that I should folk of the\n",
      "sleep. When he will like and make a circle such would not secret at is longered faint leagues back\n",
      "of from on the could guard, the importance he mists where in myself: hundreds back to their company.\n",
      "'I knew, the Shelob hard of you guided, turned was the shorse and a miles years of the Rangers was\n",
      "laid up to please-shadow and chief wonderust had raws clear of the With troke is much, grudges with\n",
      "dim, last._       Appoint, and checkage of Grey Would my lare to and breath, black only a put their\n",
      "own hope,' said Théoden far so many mountains this start. He report was lost of doubtle! Nob! '\n",
      "Treebeard, so the grey be, we moving to had better they little down says. `We are you can! He\n",
      "wonderful! Come back silent.'      `What side it. It kept and was this led up and pacced him. `I'm\n",
      "the may silver for night be company,' said Frodo, and I have you have you, fagred and move of the\n",
      "slow too singing up and smell\n"
     ]
    }
   ],
   "source": [
    "lr=1e-4 # learning rate\n",
    "weight_L2=10e-2 # L2 penalty\n",
    "max_iters=50000\n",
    "eval_interval=1000\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_L2)\n",
    "try:\n",
    "    for iter in range(max_iters+1):\n",
    "        if iter % eval_interval==0:\n",
    "            losses = estimate_loss(model, data)\n",
    "            print(f\"step {iter}: train loss = {losses['train']:.4f}, eval loss = {losses['val']:.4f}\")\n",
    "            #torch.save(model.state_dict(), model.config.MODEL_PATH[:-3]+f'_{losses['val'].item():.4f}'+'.pt')\n",
    "        x,y=get_batch(model.config, data, 'train')\n",
    "        logits, loss = model(x,y)\n",
    "        optimizer.zero_grad(set_to_none=True) # why set to none?\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    print(\"Model's output after training:\",'\\n')\n",
    "    torch.save(model.state_dict(), model.config.MODEL_PATH)\n",
    "    model.snippet(wrap=True, max_new_tokens=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ever read Tolkien, this is very recognizable!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And then Frodo finally signed up to github in order to set three ahead away-and went over of evil.\n",
      "`Then the Nardol sayëerinou,' said Dáin say; 'and the taken trumpets far feet. The used. 'So thought\n",
      "moved by a lay welcome, but one eyes. He took at thing to the ponies in the world of scome day do\n",
      "myself.      `Look!' he have reach thoughtful have meaning of the Road of Durin's her all rim, this\n",
      "hand the might host and grass the bank, and knees with a certain.'      `Hi! Sam mire is guide \n",
      "unlessed that is side a once side. 'No, the sunless Bane had been if fire, borders came hungry. He\n",
      "now by the chasm. 'Do not know no feel together two dread, they drawn find heard fell whom Gamling\n",
      "through to perhaps fall, below rettless had not seems are with the hobbits gentle far away, did\n",
      "valianted they grass. A new that there some conting, but not seemed the low jogged to his he had\n",
      "Gollum of gold figure the hads. This will and voice.      Gandalf shep under stone mane went flence.\n",
      "There was another, three jumped it weary drickery the doors; their powed the accurven his far of the\n",
      "Road. He stood the North who could come out of speaking the first that you seemed to furnaced him\n",
      "and there of the baruse my people fine. But cannot yet any with hill-tops under that its might from\n",
      "in. But now what you have did not get of him. The victory faces that eyes. Rath, and Gollum. We'll\n",
      "be out him comes deep by sight the gate shoulder. In am as the stone. Out for he king, and his\n",
      "precious awakes and was not small deadly.'      `Now have been of you. Gandalf in a king's it\n",
      "appears. We must good friendless? We shall go It was a had streams the Gates, speak sky. Sméagol the\n",
      "Hornburg hand. He was my heart; prudel the same in he stopped that you reight.'      `In it is have\n",
      "cold Tinúviel hem side, my music after laught the little leaving came at all shrill seef the Ring\n",
      "song to Recognes, and they were many are bearing from was the other own from the bones, I ren't\n",
      "called. It was he left And desprang.'      There were need sung them in the king birds and by almost\n",
      "in the last folk, and already scontried by more,' he said: `and a lot of the for your folk. `I don't\n",
      "go too eats not came name the Walls,' answered Sam trees had some together. Erkenbranches spared by\n",
      "the Great!'      `Though he cries were between very of these Black But I'Are don't know which\n",
      "trouble not a laid me. That the Captain Legolas, they forward be looks slow apple of Black Roders\n",
      "had air a Knock When hill not see to were me.'       There is like them and something and did not be\n",
      "blow touched to see she, but he ringing in they stirred in door quietly repain. We they eyes to\n",
      "people of they were clear or bowed, we haven choose are very without a never sit speak and proach\n",
      "would muttering voices of the gate again. The air when that domange could be them.      There off\n",
      "woodlessly beginner camp to him, and his eyes.'      'Orcs cry with things. 'Here turn alls and so,\n",
      "that's whispering that world, and it only the night. 'But do that is you say? ' he said. 'Tha\n"
     ]
    }
   ],
   "source": [
    "model.snippet(\"And then Frodo finally signed up to github in order to\", wrap=True, max_new_tokens=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
